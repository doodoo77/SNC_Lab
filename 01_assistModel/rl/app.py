# app.py
import os
import io
import json
import time
import uuid
import base64
import hashlib
import pathlib
from typing import Optional, List, Dict, Any
import pandas as pd
import re

import streamlit as st

# LangChain (OpenAI-í˜¸í™˜/Bedrockìš©)
from langchain_core.messages import HumanMessage, SystemMessage, AIMessage
from langchain_openai import ChatOpenAI
from langchain_core.prompts import PromptTemplate  # LangChain 0.2+

# (ì˜µì…˜) AWS Bedrockì„ LangChainìœ¼ë¡œ ì“°ë ¤ëŠ” ê²½ìš°
try:
    from langchain_aws import ChatBedrock
    BEDROCK_AVAILABLE = True
except Exception:
    BEDROCK_AVAILABLE = False


# ========================= ê³µí†µ ìœ í‹¸ =========================
def b64_data_url(image_bytes: bytes, mime="image/png") -> str:
    b64 = base64.b64encode(image_bytes).decode("utf-8")
    return f"data:{mime};base64,{b64}"

def sha256_bytes(b: bytes) -> str:
    return hashlib.sha256(b).hexdigest()

def ensure_dir(path: str) -> None:
    pathlib.Path(path).mkdir(parents=True, exist_ok=True)

EXPORT_DIR = "hf_export"
EXPORT_JSONL = f"{EXPORT_DIR}/data.jsonl"
EXPORT_IMG_DIR = f"{EXPORT_DIR}/images"

def init_state():
    ss = st.session_state
    ss.setdefault("chat", [])                 # [{"role":"ai","raw":str,"data":dict}]
    ss.setdefault("history", [])
    ss.setdefault("last_ai_id", None)
    # ì—°ê²° ìƒíƒœ ë³´ì¡´
    ss.setdefault("llm", None)
    ss.setdefault("provider_sel", None)
    ss.setdefault("model_name_sel", "")
    ss.setdefault("vertex_cfg", {})
    # ìµœê·¼ JSON ê²°ê³¼(ì „ë¬¸ê°€ í¼ ì±„ìš°ê¸°ìš©)
    ss.setdefault("last_ai_json", None)
    ss.setdefault("last_ai_raw", "")

init_state()
ensure_dir(EXPORT_DIR)
ensure_dir(EXPORT_IMG_DIR)


# ========================= LLM íŒ©í† ë¦¬ =========================
@st.cache_resource(show_spinner=False)
def make_openai_like_llm(api_key: str, model: str, base_url: Optional[str], temperature: float):
    """OpenAI-í˜¸í™˜ ì—”ë“œí¬ì¸íŠ¸(ì˜ˆ: OpenAI, Azure-OpenAI, ìì²´ í˜¸í™˜ ì„œë²„ ë“±)."""
    if not api_key:
        raise ValueError("API Keyê°€ í•„ìš”í•©ë‹ˆë‹¤.")
    # JSON ëª¨ë“œ ê°•ì œ
    return ChatOpenAI(
        api_key=api_key,
        model=model,
        base_url=base_url or None,
        temperature=temperature,
        model_kwargs={"response_format": {"type": "json_object"}},
    )

@st.cache_resource(show_spinner=False)
def make_bedrock_llm(region: str, model_id: str, temperature: float):
    """(ì˜µì…˜) AWS Bedrock. ì‚¬ì „ ìê²© ì¦ëª… í•„ìš”(AWS CLI/í™˜ê²½ë³€ìˆ˜ ë“±)."""
    if not BEDROCK_AVAILABLE:
        raise RuntimeError("langchain_aws ê°€ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.")
    # Bedrockì€ ëª¨ë¸ë³„ JSON ëª¨ë“œê°€ ë‹¤ë¥´ë¯€ë¡œ ì—¬ê¸°ì„œëŠ” ì¼ë°˜ ì„¤ì •ë§Œ.
    return ChatBedrock(
        model_id=model_id,
        region_name=region,
        model_kwargs={"temperature": temperature},
    )

from google.genai import types

# ---------- Vertex AI(Gemini) íŠœë‹ ì—”ë“œí¬ì¸íŠ¸ ì–´ëŒ‘í„° ----------
def make_vertex_endpoint_llm(project_id: str, location: str, endpoint_id: str, credentials=None):
    from google import genai
    from google.genai.types import HttpOptions

    class VertexEndpointLLM:
        def __init__(self, project: str, loc: str, eid: str, creds):
            if not (project and loc and eid):
                raise ValueError("PROJECT_ID / LOCATION / ENDPOINT_IDê°€ í•„ìš”í•©ë‹ˆë‹¤.")
            self.client = genai.Client(
                vertexai=True,
                project=project,
                location=loc,
                credentials=creds,
                http_options=HttpOptions(api_version="v1"),
            )
            self.model = f"projects/{project}/locations/{loc}/endpoints/{eid}"

        def generate(self, user_text: str, *, image_bytes: bytes | None = None,
                     mime: str | None = None, system_prompt: str | None = None) -> str:
            parts = []
            if system_prompt:
                parts.append({"text": f"[SYSTEM]\n{system_prompt}"})
            parts.append({"text": user_text})
            if image_bytes:
                parts.append({"inline_data": {"mime_type": mime or "image/png", "data": image_bytes}})
            resp = self.client.models.generate_content(
                model=self.model,
                contents=[{"role": "user", "parts": parts}],
                config=types.GenerateContentConfig(
                    response_mime_type="application/json",
                ),
            )
            return getattr(resp, "text", str(resp))

        def invoke(self, messages):
            from langchain_core.messages import SystemMessage, HumanMessage
            system_prompt, user_text = "", ""
            image_bytes, mime = None, None
            for m in messages:
                if isinstance(m, SystemMessage) and isinstance(m.content, str):
                    system_prompt += m.content
                elif isinstance(m, HumanMessage):
                    if isinstance(m.content, str):
                        user_text += m.content
                    elif isinstance(m.content, list):
                        for c in m.content:
                            if c.get("type") == "text":
                                user_text += c.get("text", "")
                            elif c.get("type") == "image_url":
                                url = c["image_url"]["url"]
                                if url.startswith("data:"):
                                    header, b64 = url.split(",", 1)
                                    mime = header.split(";")[0].split(":")[1]
                                    image_bytes = base64.b64decode(b64)
            return self.generate(user_text, image_bytes=image_bytes, mime=mime,
                                 system_prompt=system_prompt)

    return VertexEndpointLLM(project_id, location, endpoint_id, credentials)

# ========================= ê³µí†µ ëª¨ë¸ í˜¸ì¶œ(ë©€í‹°ëª¨ë‹¬) =========================
def call_llm_with_optional_image(llm, user_text: str, image_bytes: Optional[bytes]) -> str:
    """
    ë©€í‹°ëª¨ë‹¬ ë©”ì‹œì§€ ì „ì†¡. (JSON ëª¨ë“œë¡œ ì‘ë‹µí•˜ë„ë¡ í”„ë¡¬í”„íŠ¸ì—ì„œ ê°•ì œ)
    """
    if image_bytes:
        data_url = b64_data_url(image_bytes)
        human = HumanMessage(content=[
            {"type": "text", "text": user_text},
            {"type": "image_url", "image_url": {"url": data_url}},
        ])
    else:
        human = HumanMessage(content=user_text)

    sys = SystemMessage(content="You are a helpful assistant. Reply with pure JSON only.")
    ai = llm.invoke([sys, human])
    return ai.content if isinstance(ai, AIMessage) else str(ai)


# ========================= JSON íŒŒì‹±/ë Œë”ë§ =========================
def safe_json_loads(text: str) -> Optional[Dict[str, Any]]:
    """ëª¨ë¸ì´ ë§ˆí¬ë‹¤ìš´/ì„¤ëª…ì„ ì„ì–´ë„ JSON ë³¸ë¬¸ë§Œ ì¶”ì¶œí•´ì„œ íŒŒì‹±."""
    # ì½”ë“œíœìŠ¤/ë¶ˆìˆœë¬¼ ì œê±°
    cleaned = text.strip()
    cleaned = re.sub(r"^```(?:json)?|```$", "", cleaned, flags=re.MULTILINE).strip()
    try:
        return json.loads(cleaned)
    except Exception:
        # ê°€ì¥ ë°”ê¹¥ {} ë¸”ëŸ­ë§Œ ì¡ê¸°
        m = re.search(r"\{[\s\S]*\}\s*$", cleaned)
        if m:
            try:
                return json.loads(m.group(0))
            except Exception:
                return None
    return None

def render_result(data: Dict[str, Any], raw_text: str | None = None):
    checks = data.get("checks", {})
    fix = data.get("fix", {})
    reasoning = data.get("reasoning", [])

    with st.container(border=True):
        # 1) ğŸ” ì¶”ë¡ (ìš”ì•½) â€” ì œì¼ ìœ„ì—, ì ‘ì§€ ì•Šê³  ë°”ë¡œ í‘œì‹œ
        if reasoning:
            st.markdown("### ğŸ” ì¶”ë¡ (ìš”ì•½)")
            for i, r in enumerate(reasoning, 1):
                st.markdown(f"{i}. {r}")
            st.markdown("---")  # ì¶”ë¡ ê³¼ ì§„ë‹¨ ê²°ê³¼ ì‚¬ì´ êµ¬ë¶„ì„ 

        # 2) âœ… ì§„ë‹¨ ê²°ê³¼
        st.markdown("### âœ… ì§„ë‹¨ ê²°ê³¼")
        col1, col2 = st.columns(2)
        with col1:
            st.markdown("**ê²€ì‚¬í•­ëª©**")
            st.write(checks.get("ê²€ì‚¬í•­ëª©", ""))
        with col2:
            st.markdown("**ì˜¤ë¥˜ìœ í˜•**")
            st.write(checks.get("ì˜¤ë¥˜ìœ í˜•", ""))

        st.markdown("**ê°œì„ ë°©ì•ˆ(ì„¤ëª…)**")
        st.write(fix.get("text", ""))

        if fix.get("code_html"):
            st.markdown("**ê°œì„ ë°©ì•ˆ(ì½”ë“œ)**")
            st.code(fix["code_html"], language="html")

        # 3) ğŸ“„ ëª¨ë¸ ì‘ë‹µ ì „ì²´(JSON) â€” ì´ê±´ ê·¸ëŒ€ë¡œ ì ‘ì–´ ë‘ê¸°
        if raw_text:
            with st.expander("ğŸ“„ ëª¨ë¸ ì‘ë‹µ ì „ì²´(JSON)", expanded=False):
                st.code(raw_text, language="json")




# ========================= JSONL ë ˆì½”ë“œ =========================
def build_record(
    *,
    model_text_original: str,
    model_text_edited: str,
    feedback_score: Optional[int],
    feedback_comment: Optional[str],
    model_name: str,
    image_meta: Optional[Dict[str, Any]],
) -> Dict[str, Any]:
    rec_id = str(uuid.uuid4())
    return {
        "id": rec_id,
        "ts": int(time.time()),
        "model_name": model_name,
        "model_text_original": model_text_original,  # JSON ë¬¸ìì—´
        "model_text_edited": model_text_edited,      # JSON ë¬¸ìì—´(í¸ì§‘ë³¸)
        "feedback_score": feedback_score,            # 1~5
        "feedback_comment": feedback_comment or "",
        "image": image_meta or {},                   # {"path": "...", "sha256": "...", "mime": "..."}
    }

def append_jsonl(path: str, record: Dict[str, Any]) -> None:
    ensure_dir(str(pathlib.Path(path).parent))
    with open(path, "a", encoding="utf-8") as f:
        f.write(json.dumps(record, ensure_ascii=False) + "\n")


# ========================= ì‚¬ì´ë“œë°”(ëª¨ë¸ ì„¤ì •) =========================
st.sidebar.header("ğŸ”§ ëª¨ë¸ ì„¤ì •")
provider = st.sidebar.selectbox(
    "Provider",
    ["OpenAI-compatible", "AWS Bedrock", "Vertex AI (Gemini Endpoint)"]
)
temperature = st.sidebar.slider("temperature", 0.0, 1.0, 0.2, 0.1)

llm = None
model_name = ""

if provider == "OpenAI-compatible":
    api_key = st.sidebar.text_input("API Key", type="password")
    base_url = st.sidebar.text_input("Base URL (ì„ íƒ)", help="OpenAI-í˜¸í™˜ ì„œë²„ì˜ ì—”ë“œí¬ì¸íŠ¸ê°€ ìˆìœ¼ë©´ ì…ë ¥")
    model_name = st.sidebar.text_input("Model", value="gpt-4o-mini")
    if st.sidebar.button("ğŸ”Œ Connect", use_container_width=True):
        try:
            llm = make_openai_like_llm(api_key, model_name, base_url, temperature)
            st.sidebar.success("ì—°ê²° ì„±ê³µ")
        except Exception as e:
            st.sidebar.error(f"ì—°ê²° ì‹¤íŒ¨: {e}")
    else:
        if api_key and model_name:
            try:
                llm = make_openai_like_llm(api_key, model_name, base_url, temperature)
            except Exception:
                pass

elif provider == "AWS Bedrock":
    region = st.sidebar.text_input("AWS Region", value="us-east-1")
    model_name = st.sidebar.text_input("Model ID", value="anthropic.claude-3-5-sonnet-20241022-v2:0")
    if st.sidebar.button("ğŸ”Œ Connect", use_container_width=True):
        try:
            llm = make_bedrock_llm(region, model_name, temperature)
            st.sidebar.success("ì—°ê²° ì„±ê³µ")
        except Exception as e:
            st.sidebar.error(f"ì—°ê²° ì‹¤íŒ¨: {e}")

elif provider == "Vertex AI (Gemini Endpoint)":
    from google.oauth2 import service_account
    import json as pyjson

    st.sidebar.markdown("íŠœë‹ëœ **Gemini 2.5 Pro** ì—”ë“œí¬ì¸íŠ¸ë¥¼ ì§€ì •í•˜ì„¸ìš”.")
    project_id  = st.sidebar.text_input("PROJECT_ID",  value="",        key="vx_project_id")
    location    = st.sidebar.text_input("LOCATION",    value="us-central1", key="vx_location")
    endpoint_id = st.sidebar.text_input("ENDPOINT_ID", value="",        key="vx_endpoint_id")

    # â–¶ ì„œë¹„ìŠ¤ ê³„ì • JSON ì—…ë¡œë“œ(ì„ íƒ)
    with st.sidebar.expander("Google ì¸ì¦(ì„œë¹„ìŠ¤ ê³„ì • JSON ì—…ë¡œë“œ)", expanded=False):
        sa_file = st.file_uploader("service-account.json", type=["json"], key="vx_sa_json")
        creds = None
        if sa_file is not None:
            sa_info = pyjson.loads(sa_file.getvalue())
            creds = service_account.Credentials.from_service_account_info(
                sa_info, scopes=["https://www.googleapis.com/auth/cloud-platform"]
            )
            st.sidebar.success("ì„œë¹„ìŠ¤ ê³„ì • ë¡œë“œ ì™„ë£Œ")

    model_name = endpoint_id or "vertex-gemini-endpoint"

    if st.sidebar.button("ğŸ”Œ Connect", use_container_width=True, key="vx_connect_btn"):
        try:
            llm = make_vertex_endpoint_llm(project_id, location, endpoint_id, credentials=creds)
            st.sidebar.success("Vertex ì—”ë“œí¬ì¸íŠ¸ ì—°ê²° ì„±ê³µ")
            # ì„¸ì…˜ì— ë³´ì¡´(ì¬ì‹¤í–‰ ëŒ€ë¹„)
            st.session_state.llm = llm
            st.session_state.provider_sel = "Vertex AI (Gemini Endpoint)"
            st.session_state.model_name_sel = model_name
            st.session_state.vertex_cfg = {
                "project": project_id, "location": location, "endpoint": endpoint_id
            }
        except Exception as e:
            st.sidebar.error(f"ì—°ê²° ì‹¤íŒ¨: {e}")


# ========================= ë³¸ë¬¸ UI =========================
st.title("ì ‘ê·¼ì„± ì§„ë‹¨ ëŠ¥ë ¥ í‰ê°€ í˜ì´ì§€")
st.caption("ì´ë¯¸ì§€ ì…ë ¥, ì™¸ë¶€ ì—”ë“œí¬ì¸íŠ¸ í˜¸ì¶œ, JSON êµ¬ì¡°í™”, ì „ë¬¸ê°€ í”¼ë“œë°±, JSONL ì €ì¥ì„ í¬í•¨í•©ë‹ˆë‹¤.")

# 1) ìœ ì € ì…ë ¥
with st.container(border=True):
    st.subheader("ì…ë ¥")

    # 1) ì˜¤ë¥˜ ì˜ì—­ ì´ë¯¸ì§€ ì—…ë¡œë“œ
    with st.expander("ğŸ“ ì˜¤ë¥˜ ì˜ì—­ ì´ë¯¸ì§€ ì—…ë¡œë“œ", expanded=False):
        uploaded_img = st.file_uploader("ì˜¤ë¥˜ ì˜ì—­ ì´ë¯¸ì§€ ì—…ë¡œë“œ (ì„ íƒ)", type=["png", "jpg", "jpeg", "webp"])

    # 2) í‘œì¤€ ê°œì„ ë°©ì•ˆ ë¦¬ìŠ¤íŠ¸(Excel) ì—…ë¡œë“œ
    standard_texts_str = ""
    std_rows_count = 0
    with st.expander("ğŸ“ í‘œì¤€ ê°œì„ ë°©ì•ˆ ë¦¬ìŠ¤íŠ¸(Excel) ì—…ë¡œë“œ", expanded=False):
        std_file = st.file_uploader("í‘œì¤€ ê°œì„ ë°©ì•ˆ Excel (.xlsx/.xls)", type=["xlsx", "xls"], key="std_xlsx")
        if std_file is not None:
            try:
                xls = pd.ExcelFile(std_file)
                sheet_sel = st.selectbox("ì‹œíŠ¸ ì„ íƒ", xls.sheet_names, key="std_sheet_sel")
                df = xls.parse(sheet_sel)

                st.caption("ë¯¸ë¦¬ë³´ê¸° (ìƒìœ„ 10í–‰)")
                st.dataframe(df.head(10), use_container_width=True)

                cols = st.multiselect("í¬í•¨í•  ì—´ ì„ íƒ", list(df.columns), default=list(df.columns), key="std_cols")
                max_rows = st.slider("í¬í•¨í•  ìµœëŒ€ í–‰ ìˆ˜", 10, 2000, 300, step=10, key="std_max_rows")

                df_use = df[cols] if cols else df
                df_use = df_use.head(max_rows)
                std_rows_count = len(df_use)

                records = df_use.to_dict(orient="records")
                standard_texts_str = json.dumps(records, ensure_ascii=False)

                st.caption(f"ëª¨ë¸ì— ì „ë‹¬ë  í‘œì¤€ í…ìŠ¤íŠ¸ (ì´ {std_rows_count}í–‰)")
                st.text_area(
                    "ì „ë‹¬ ë³¸ë¬¸(ì½ê¸° ì „ìš©, í† í° ì ˆì•½ì„ ìœ„í•´ ì—´/í–‰ì„ ì¡°ì ˆí•˜ì„¸ìš”)",
                    value=standard_texts_str[:10000],
                    height=160,
                    disabled=True,
                    key="std_preview",
                )
            except Exception as e:
                st.warning(f"ì—‘ì…€ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
        else:
            standard_texts_str = ""

    # 3) ì˜¤ë¥˜ ì˜ì—­ ì½”ë“œ ì…ë ¥
    error_code_str = st.text_area("ì˜¤ë¥˜ ì˜ì—­ ì½”ë“œ", value="", height=220, key="err_code_text")

    # 4) ì „ë¬¸ê°€ ë©”ëª¨
    memo_str = st.text_area("ì „ë¬¸ê°€ ë©”ëª¨", placeholder="ì§„ë‹¨ì— ë„ì›€ë˜ëŠ” ë§¥ë½/íŠ¹ì´ì‚¬í•­ ë“±ì„ ë©”ëª¨í•˜ì„¸ìš”.", height=120, key="expert_memo")

    # 5) ë²„íŠ¼
    c1, c2 = st.columns([1,1])
    with c1:
        run_btn = st.button("ëª¨ë¸ í˜¸ì¶œ", use_container_width=True)
    with c2:
        clear_btn = st.button("ëŒ€í™” ì´ˆê¸°í™”", use_container_width=True)

if clear_btn:
    st.session_state.chat.clear()
    st.session_state.last_ai_id = None
    st.session_state.last_ai_json = None
    st.rerun()

llm = st.session_state.llm
active_provider = st.session_state.provider_sel or provider
model_name = st.session_state.model_name_sel or model_name  # ê¸°ë¡ìš©

# 2) ëª¨ë¸ í˜¸ì¶œ
if run_btn:
    if llm is None:
        st.error("ì‚¬ì´ë“œë°”ì—ì„œ ëª¨ë¸ ì—°ê²° ì •ë³´ë¥¼ ì…ë ¥/ì—°ê²°í•˜ì„¸ìš”.")
    else:
        # --- ì ‘ê·¼ì„± í‰ê°€ ìë™ í”„ë¡¬í”„íŠ¸ (JSON ê°•ì œ) ---
        A11Y_PROMPT = r"""[[ì—­í• ]
        ë„ˆëŠ” ì ‘ê·¼ì„± í‰ê°€ ì „ë¬¸ê°€ë‹¤.

        [ì…ë ¥]
        ì „ì²´ í˜ì´ì§€ ìŠ¤í¬ë¦°ìƒ· -
        ì˜¤ë¥˜ ì˜ì—­ ìŠ¤í¬ë¦°ìƒ· -  
        í‘œì¤€ ê°œì„ ë°©ì•ˆ ë¦¬ìŠ¤íŠ¸ - {standard_texts}
        ì˜¤ë¥˜ ì˜ì—­ ì½”ë“œ - {error_code}
        ì¸ê°„ ì „ë¬¸ê°€ê°€ ì‘ì„±í•œ ë©”ëª¨ - {memo}

        [ì§€ì‹œë¬¸]
        1) ë¨¼ì € [ì˜¤ë¥˜ ì˜ì—­ ìŠ¤í¬ë¦°ìƒ·/ì„¤ëª…]ì´ ìœ„ë°˜í•œ ì ‘ê·¼ì„± ì˜¤ë¥˜ ìœ í˜•ì„ íŒë‹¨í•˜ê³ ,
        [í‘œì¤€ ê°œì„ ë°©ì•ˆ ë¦¬ìŠ¤íŠ¸]ì—ì„œ **ê°€ì¥ ê´€ë ¨ ìˆëŠ” í•­ëª©ì˜ â€œê²€ì‚¬í•­ëª©â€ê³¼ â€œì˜¤ë¥˜ìœ í˜•â€ í…ìŠ¤íŠ¸ë¥¼ ê·¸ëŒ€ë¡œ ì¸ìš©**í•´.

        2) â€œë¬¸ì œì  ë° ê°œì„ ë°©ì•ˆâ€ì„ ì‘ì„±í•´.
        - _í…ìŠ¤íŠ¸_: ì™œ ë¬¸ì œì¸ì§€ + í‘œì¤€ ì¶©ì¡±ì„ ìœ„í•´ ì–´ë–»ê²Œ ì½”ë“œê°€ ìˆ˜ì •ë˜ì–´ì•¼ í•˜ëŠ”ì§€ ì‰¬ìš´ ë¬¸ì¥ìœ¼ë¡œ ì¥í™©í•˜ì§€ ì•Šìœ¼ë©´ì„œ í•µì‹¬ì ì¸ ë‚´ìš©ì„ í¬í•¨í•´ì„œ ì„¤ëª….
        - _ì½”ë“œ(ì„ íƒ)_: {error_code}ê°€ ì£¼ì–´ì¡Œë‹¤ë©´, í•´ë‹¹ ì˜¤ë¥˜ë¥¼ ì¤€ìˆ˜í•˜ê¸° ìœ„í•´ì„œ ìˆ˜ì •ë˜ì–´ì•¼ í•  ì½”ë“œë¥¼ ì œì‹œí•˜ë©´ ë¼.

        3) ì§„ë‹¨ ì „ í•„ìˆ˜ ì¶”ë¡  ì ˆì°¨:
        - [ì „ì²´ í˜ì´ì§€ ìŠ¤í¬ë¦°ìƒ·/ì„¤ëª…]ìœ¼ë¡œ í˜ì´ì§€ ëª©ì  1ì¤„ íŒŒì•…
        - ê·¸ ëª©ì ì„ ì°¸ê³ í•˜ì—¬ [ì˜¤ë¥˜ ì˜ì—­ ìŠ¤í¬ë¦°ìƒ·/ì„¤ëª…]ì˜ ì½˜í…ì¸  ì—­í•  íŒŒì•…
        - {error_code}ê¹Œì§€ ê³ ë ¤í•´ ìµœì¢… ì§„ë‹¨ ì‘ì„±
        - ì•„ë˜ ëª…ì‹œëœ ìì œê²€ì¦ ì²´í¬ë¦¬ìŠ¤íŠ¸ì— ë¶€í•©ë ë•Œê¹Œì§€ ì¶”ë¡  ê³¼ì •ì„ ë°˜ë³µí•´

        [ì¶œë ¥ í˜•ì‹ â€” JSONë§Œ, í•œêµ­ì–´, ë§ˆí¬ë‹¤ìš´/ì„¤ëª…/ì½”ë“œíœìŠ¤ ê¸ˆì§€]
        {% raw %}
        {
        "reasoning": ["í•µì‹¬ ì¶”ë¡  1", "í•µì‹¬ ì¶”ë¡  2"],
        "checks": { "ê²€ì‚¬í•­ëª©": "<í‘œì¤€ ì¸ìš©>", "ì˜¤ë¥˜ìœ í˜•": "<í‘œì¤€ ì¸ìš©>" },
        "fix": { "text": "ê°œì„ ë°©ì•ˆ ì„¤ëª…", "code_html": "<ìˆ˜ì • ì˜ˆì‹œ HTML ë˜ëŠ” ë¹ˆ ë¬¸ìì—´>" }
        }
        {% endraw %}
        """
        prompt_tmpl = PromptTemplate(template=A11Y_PROMPT, template_format="jinja2")
        combined_text = prompt_tmpl.format(
            standard_texts=standard_texts_str or "",
            error_code=error_code_str or "",
            memo=memo_str or "",
        )

        image_bytes = uploaded_img.read() if uploaded_img else None
        try:
            if active_provider == "Vertex AI (Gemini Endpoint)":
                mime = (uploaded_img.type if uploaded_img and hasattr(uploaded_img, "type") else None)
                ai_text = llm.generate(
                    combined_text,
                    image_bytes=image_bytes,
                    mime=mime,
                    system_prompt="Reply with pure JSON only."
                )
            else:
                ai_text = call_llm_with_optional_image(llm, combined_text, image_bytes)
        except Exception as e:
            st.error(f"ëª¨ë¸ í˜¸ì¶œ ì‹¤íŒ¨: {e}")
            ai_text = ""

        st.session_state.last_ai_raw = ai_text

        # JSON íŒŒì‹±
        data = safe_json_loads(ai_text) if ai_text else None
        if not data:
            st.warning("ëª¨ë¸ì´ JSON í˜•ì‹ì„ ë”°ë¥´ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì•„ë˜ ì›ë¬¸ ì‘ë‹µì„ ì°¸ê³ í•˜ì„¸ìš”.")
            st.session_state.last_ai_json = None
        else:
            # ê²°ê³¼ëŠ” ì„¸ì…˜ì—ë§Œ ì €ì¥(ë Œë”ë§ì€ ì•„ë˜ ê³µí†µ ì„¹ì…˜ì—ì„œ í•œ ë²ˆë§Œ)
            st.session_state.last_ai_json = data

        # ì´ë¯¸ì§€ ì €ì¥(ìˆë‹¤ë©´) - ë©”íƒ€ ê¸°ë¡ìš©
        image_meta = None
        if image_bytes:
            img_id = str(uuid.uuid4())
            ext = pathlib.Path(uploaded_img.name).suffix.lower() or ".png"
            img_path = f"{EXPORT_IMG_DIR}/{img_id}{ext}"
            with open(img_path, "wb") as f:
                f.write(image_bytes)
            image_meta = {"path": img_path}
            try:
                image_meta["sha256"] = sha256_bytes(image_bytes)
                image_meta["mime"] = uploaded_img.type if hasattr(uploaded_img, "type") else "image/*"
            except Exception:
                pass

        # ì±„íŒ… íƒ€ì„ë¼ì¸ì—ëŠ” AIë§Œ ë³´ì¡´
        st.session_state.chat.append({"role": "ai", "raw": ai_text, "data": data or None})
        st.session_state.last_ai_id = len(st.session_state.chat) - 1


# 3) ì§„ë‹¨ ê²°ê³¼ + ì¶”ë¡ /ì›ë¬¸ ì¶œë ¥(ìµœê·¼ 1ê°œë§Œ)
if st.session_state.last_ai_json or st.session_state.last_ai_raw:
    with st.chat_message("assistant"):
        if st.session_state.last_ai_json:
            # JSON íŒŒì‹± ì„±ê³µ: ì¹´ë“œ + ì¶”ë¡  + JSON expander
            render_result(
                st.session_state.last_ai_json,
                raw_text=st.session_state.last_ai_raw,
            )
        else:
            # JSON íŒŒì‹± ì‹¤íŒ¨: ì›ë¬¸ë§Œ í‘œì‹œ
            st.write(st.session_state.last_ai_raw)


# 4) ì „ë¬¸ê°€ ê²€ì¦/í¸ì§‘/í”¼ë“œë°± (JSONì„ í¼ì— ìë™ ì£¼ì…)
if st.session_state.last_ai_id is not None:
    ai_idx = st.session_state.last_ai_id
    ai_raw = st.session_state.chat[ai_idx].get("raw", "")
    ai_data = st.session_state.chat[ai_idx].get("data", None) or st.session_state.last_ai_json

    with st.container(border=True):
        st.subheader("ì „ë¬¸ê°€ í”¼ë“œë°±")

        # 1. ì¶”ë¡  ë¨¼ì € í¸ì§‘
        st.markdown("#### 1. ì¶”ë¡  ìˆ˜ì •")
        f_reasoning = st.text_area(
            "ì¶”ë¡ (í•œ ì¤„ë‹¹ í•˜ë‚˜, ìµœëŒ€ 5ê°œ ê¶Œì¥)",
            value="\n".join((ai_data or {}).get("reasoning", [])),
            height=140,
        )

        # 2. ì§„ë‹¨ í•­ëª© (ê²€ì‚¬í•­ëª© / ì˜¤ë¥˜ìœ í˜•)
        st.markdown("#### 2. ì§„ë‹¨ í•­ëª© ìˆ˜ì •")
        checks_col1, checks_col2 = st.columns(2)
        with checks_col1:
            f_check_item = st.text_input(
                "ê²€ì‚¬í•­ëª©",
                value=(ai_data or {}).get("checks", {}).get("ê²€ì‚¬í•­ëª©", ""),
            )
        with checks_col2:
            f_check_type = st.text_input(
                "ì˜¤ë¥˜ìœ í˜•",
                value=(ai_data or {}).get("checks", {}).get("ì˜¤ë¥˜ìœ í˜•", ""),
            )

        # 3. ê°œì„ ë°©ì•ˆ (ì„¤ëª… / ì½”ë“œ)
        st.markdown("#### 3. ê°œì„ ë°©ì•ˆ ìˆ˜ì •")
        f_fix_text = st.text_area(
            "ê°œì„ ë°©ì•ˆ(ì„¤ëª…)",
            value=(ai_data or {}).get("fix", {}).get("text", ""),
            height=140,
        )
        f_fix_code = st.text_area(
            "ê°œì„ ë°©ì•ˆ(ì½”ë“œ, HTMLë§Œ)",
            value=(ai_data or {}).get("fix", {}).get("code_html", ""),
            height=160,
        )

        # 4. í”¼ë“œë°± ì ìˆ˜ Â· ì½”ë©˜íŠ¸ Â· ì €ì¥ ë²„íŠ¼
        st.markdown("#### 4. í”¼ë“œë°±")
        cA, cB, cC = st.columns([1, 2, 1])
        with cA:
            score = st.radio("ë§Œì¡±ë„ ì ìˆ˜", [1, 2, 3, 4, 5], index=3, horizontal=True)
        with cB:
            comment = st.text_input("ì½”ë©˜íŠ¸(ì„ íƒ)", placeholder="ì™œ ë§Œì¡±/ë¶ˆë§Œì¡±ì¸ì§€, ìˆ˜ì • ì´ìœ  ë“±")
        with cC:
            save_btn = st.button("ğŸ“ í”¼ë“œë°± ì €ì¥\n(JSONLì— ì¶”ê°€)", use_container_width=True)

        if save_btn:
            # í¸ì§‘ë³¸ JSON ì¡°ë¦½
            edited_json = {
                "reasoning": [s.strip() for s in f_reasoning.split("\n") if s.strip()],
                "checks": {"ê²€ì‚¬í•­ëª©": f_check_item, "ì˜¤ë¥˜ìœ í˜•": f_check_type},
                "fix": {"text": f_fix_text, "code_html": f_fix_code},
            }
            edited_str = json.dumps(edited_json, ensure_ascii=False)

            image_meta = None  # (ì´ë¯¸ì§€ ë©”íƒ€ëŠ” í•„ìš”ì‹œ ì—¬ê¸°ì— ì—°ê²°)

            rec = build_record(
                model_text_original=ai_raw,
                model_text_edited=edited_str if edited_str != ai_raw else "",
                feedback_score=int(score),
                feedback_comment=comment or "",
                model_name=model_name,
                image_meta=image_meta,
            )
            st.session_state.history.append(rec)
            append_jsonl(EXPORT_JSONL, rec)
            st.success(f"ì €ì¥ ì™„ë£Œ: {EXPORT_JSONL}")

            with st.expander("ì €ì¥ëœ ë ˆì½”ë“œ ë¯¸ë¦¬ë³´ê¸°"):
                st.json(rec)


# 5) ë‚´ë³´ë‚´ê¸° / í—ˆê¹…í˜ì´ìŠ¤ ì—…ë¡œë“œ
with st.container(border=True):
    st.subheader("ğŸ“¦ ë°ì´í„°ì…‹ ë‚´ë³´ë‚´ê¸°")
    st.caption("`hf_export/data.jsonl` íŒŒì¼ì— ìë™ ëˆ„ì  ì €ì¥ë©ë‹ˆë‹¤. í—ˆê¹…í˜ì´ìŠ¤ ë°ì´í„°ì…‹ìœ¼ë¡œ ë°”ë¡œ pushí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    c1, c2 = st.columns([1,1])
    with c1:
        if pathlib.Path(EXPORT_JSONL).exists():
            with open(EXPORT_JSONL, "rb") as f:
                st.download_button("data.jsonl ë‹¤ìš´ë¡œë“œ", f, file_name="data.jsonl", mime="application/jsonl")
        else:
            st.info("ì•„ì§ ì €ì¥ëœ ë ˆì½”ë“œê°€ ì—†ìŠµë‹ˆë‹¤.")
    with c2:
        st.write(f"ì €ì¥ ê²½ë¡œ: `{EXPORT_JSONL}`")

    with st.expander("í—ˆê¹…í˜ì´ìŠ¤ í—ˆë¸Œë¡œ ì—…ë¡œë“œ(ì„ íƒ)"):
        from huggingface_hub import HfApi, HfFolder, create_repo, upload_file
        repo_id = st.text_input("repo_id (org/name)", placeholder="your-org/your-dataset")
        hf_token = st.text_input("HF_TOKEN", type="password")
        path_in_repo = st.text_input("ê²½ë¡œ(ë¦¬í¬ ë‚´)", value="data/data.jsonl")
        private = st.checkbox("ë¹„ê³µê°œë¡œ ìƒì„±", value=True)
        do_push = st.button("â¬†ï¸ Push to Hub")

        if do_push:
            if not (repo_id and hf_token and pathlib.Path(EXPORT_JSONL).exists()):
                st.error("repo_id, HF_TOKEN, data.jsonl ê²½ë¡œë¥¼ í™•ì¸í•˜ì„¸ìš”.")
            else:
                try:
                    HfFolder.save_token(hf_token)
                    try:
                        create_repo(repo_id, token=hf_token, private=private, repo_type="dataset")
                    except Exception:
                        pass  # ì´ë¯¸ ì¡´ì¬
                    upload_file(
                        path_or_fileobj=EXPORT_JSONL,
                        path_in_repo=path_in_repo,
                        repo_id=repo_id,
                        repo_type="dataset",
                        token=hf_token,
                    )
                    st.success(f"Hugging Face Hub ì—…ë¡œë“œ ì™„ë£Œ: {repo_id} / {path_in_repo}")
                except Exception as e:
                    st.error(f"ì—…ë¡œë“œ ì‹¤íŒ¨: {e}")
